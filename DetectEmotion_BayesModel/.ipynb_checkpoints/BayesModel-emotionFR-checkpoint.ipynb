{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "import os\n",
    "from time import time\n",
    "from pprint import pprint\n",
    "import collections\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.set(font_scale=1.3)\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = pd.read_pickle('../data/pickle_emotion/df_model_en_.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_mentions</th>\n",
       "      <th>count_hashtags</th>\n",
       "      <th>count_capital_words</th>\n",
       "      <th>count_excl_quest_marks</th>\n",
       "      <th>count_urls</th>\n",
       "      <th>count_emojis</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>origin_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>fear</td>\n",
       "      <td>Now #India is #afraid of #bad .</td>\n",
       "      <td>india afraid bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>anger</td>\n",
       "      <td>He's seriously so frustrating sometimes! (‚ïØ¬∞‚ñ°¬∞Ôºâ‚ïØÔ∏µ ‚îª‚îÅ‚îª #ugh #raging</td>\n",
       "      <td>seriously frustrating sometimes ‚ïØ¬∞‚ñ°¬∞Ôºâ‚ïØÔ∏µ ‚îª‚îÅ‚îª ugh raging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "      <td>How in the world did Nicole beat Paul?!?! #terrible #bb18 #BBFinale What was the jury thinking?? üò≥üò≥üò≥</td>\n",
       "      <td>world nicole beat paul terrible bb bbfinale jury thinking üò≥üò≥üò≥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>anger</td>\n",
       "      <td>Yeah, no. I don' mind tweetin' that twice. #snap #blm #likeThat</td>\n",
       "      <td>yeah no mind tweetin twice snap blm likethat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>joy</td>\n",
       "      <td>What's the most important thing you need to accomplish today? #onlinemarketing #solopreneur #start up #success</td>\n",
       "      <td>important thing need accomplish today onlinemarketing solopreneur start success</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count_words  count_mentions  count_hashtags  count_capital_words  \\\n",
       "0  6            0               3               0                     \n",
       "1  8            0               2               0                     \n",
       "2  16           0               3               0                     \n",
       "3  11           0               3               0                     \n",
       "4  16           0               4               0                     \n",
       "\n",
       "   count_excl_quest_marks  count_urls  count_emojis sentiment  \\\n",
       "0  0                       0           0             fear       \n",
       "1  1                       0           0             anger      \n",
       "2  6                       0           3             anger      \n",
       "3  0                       0           0             anger      \n",
       "4  1                       0           0             joy        \n",
       "\n",
       "                                                                                                      origin_text  \\\n",
       "0  Now #India is #afraid of #bad .                                                                                  \n",
       "1  He's seriously so frustrating sometimes! (‚ïØ¬∞‚ñ°¬∞Ôºâ‚ïØÔ∏µ ‚îª‚îÅ‚îª #ugh #raging                                               \n",
       "2  How in the world did Nicole beat Paul?!?! #terrible #bb18 #BBFinale What was the jury thinking?? üò≥üò≥üò≥             \n",
       "3  Yeah, no. I don' mind tweetin' that twice. #snap #blm #likeThat                                                  \n",
       "4  What's the most important thing you need to accomplish today? #onlinemarketing #solopreneur #start up #success   \n",
       "\n",
       "                                                                        clean_text  \n",
       "0  india afraid bad                                                                 \n",
       "1  seriously frustrating sometimes ‚ïØ¬∞‚ñ°¬∞Ôºâ‚ïØÔ∏µ ‚îª‚îÅ‚îª ugh raging                           \n",
       "2  world nicole beat paul terrible bb bbfinale jury thinking üò≥üò≥üò≥                    \n",
       "3  yeah no mind tweetin twice snap blm likethat                                     \n",
       "4  important thing need accomplish today onlinemarketing solopreneur start success  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_model.drop('sentiment', axis=1), df_model.sentiment, test_size=0.1, random_state=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnExtractor(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        return X[self.cols]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_vect(clf, parameters_clf, X_train, X_test, parameters_text=None, vect=None):\n",
    "    \n",
    "    textcountscols = ['count_capital_words','count_emojis','count_excl_quest_marks','count_hashtags'\n",
    "                      ,'count_mentions','count_urls','count_words']\n",
    "    \n",
    "\n",
    "    features = FeatureUnion([('textcounts', ColumnExtractor(cols=textcountscols))\n",
    "                            , ('pipe', Pipeline([('cleantext', ColumnExtractor(cols='clean_text')), ('vect', vect)]))]\n",
    "                            , n_jobs=-1)\n",
    "\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('features', features)\n",
    "        , ('clf', clf)\n",
    "    ])\n",
    "    \n",
    "    # Join the parameters dictionaries together\n",
    "    parameters = dict()\n",
    "    if parameters_text:\n",
    "        parameters.update(parameters_text)\n",
    "    parameters.update(parameters_clf)\n",
    "\n",
    "    # initiate gridsearchCV with parameters and pipline\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, cv=5)\n",
    "    \n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "\n",
    "    t0 = time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best CV score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        \n",
    "    print(\"Test score with best_estimator_: %0.3f\" % grid_search.best_estimator_.score(X_test, y_test))\n",
    "    print(\"\\n\")\n",
    "    print(\"Classification Report Test Data\")\n",
    "    print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))\n",
    "\n",
    "    print(\"all results\")\n",
    "    means = grid_search.cv_results_['mean_test_score']\n",
    "    stds = grid_search.cv_results_['std_test_score']\n",
    "    params = grid_search.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "    \n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid settings for the vectorizers (Count and TFIDF)\n",
    "parameters_vect = {\n",
    "    'features__pipe__vect__max_df': (0.25, 0.5, 0.75),\n",
    "    'features__pipe__vect__ngram_range': ((1, 1), (1, 2)),\n",
    "    'features__pipe__vect__min_df': (1,2)\n",
    "}\n",
    "\n",
    "# Parameter grid settings for MultinomialNB\n",
    "parameters_mnb = {\n",
    "    'clf__alpha': (0.25, 0.5, 0.75)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['features', 'clf']\n",
      "parameters:\n",
      "{'clf__alpha': (0.25, 0.5, 0.75),\n",
      " 'features__pipe__vect__max_df': (0.25, 0.5, 0.75),\n",
      " 'features__pipe__vect__min_df': (1, 2),\n",
      " 'features__pipe__vect__ngram_range': ((1, 1), (1, 2))}\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed:   24.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 24.807s\n",
      "\n",
      "Best CV score: 0.571\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.75\n",
      "\tfeatures__pipe__vect__max_df: 0.25\n",
      "\tfeatures__pipe__vect__min_df: 2\n",
      "\tfeatures__pipe__vect__ngram_range: (1, 1)\n",
      "Test score with best_estimator_: 0.562\n",
      "\n",
      "\n",
      "Classification Report Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.58      0.87      0.70       284\n",
      "anticipation       0.30      0.14      0.19        95\n",
      "     disgust       0.00      0.00      0.00        48\n",
      "        fear       0.56      0.21      0.31        66\n",
      "         joy       0.59      0.77      0.67       187\n",
      "        love       0.00      0.00      0.00         1\n",
      "    optimism       0.00      0.00      0.00        22\n",
      "   pessimism       0.00      0.00      0.00        16\n",
      "     sadness       0.00      0.00      0.00        22\n",
      "    surprise       0.00      0.00      0.00         3\n",
      "\n",
      "   micro avg       0.56      0.56      0.56       744\n",
      "   macro avg       0.20      0.20      0.19       744\n",
      "weighted avg       0.46      0.56      0.49       744\n",
      "\n",
      "all results\n",
      "0.556602 (0.009848) with: {'clf__alpha': 0.25, 'features__pipe__vect__max_df': 0.25, 'features__pipe__vect__min_df': 1, 'features__pipe__vect__ngram_range': (1, 1)}\n",
      "0.540153 (0.012140) with: {'clf__alpha': 0.25, 'features__pipe__vect__max_df': 0.25, 'features__pipe__vect__min_df': 1, 'features__pipe__vect__ngram_range': (1, 2)}\n",
      "0.558397 (0.010251) with: {'clf__alpha': 0.25, 'features__pipe__vect__max_df': 0.25, 'features__pipe__vect__min_df': 2, 'features__pipe__vect__ngram_range': (1, 1)}\n",
      "0.562883 (0.009866) with: {'clf__alpha': 0.25, 'features__pipe__vect__max_df': 0.25, 'features__pipe__vect__min_df': 2, 'features__pipe__vect__ngram_range': (1, 2)}\n",
      "0.556602 (0.009848) with: {'clf__alpha': 0.25, 'features__pipe__vect__max_df': 0.5, 'features__pipe__vect__min_df': 1, 'features__pipe__vect__ngram_range': (1, 1)}\n",
      "0.540153 (0.012140) with: {'clf__alpha': 0.25, 'features__pipe__vect__max_df': 0.5, 'features__pipe__vect__min_df': 1, 'features__pipe__vect__ngram_range': (1, 2)}\n",
      "0.558397 (0.010251) with: {'clf__alpha': 0.25, 'features__pipe__vect__max_df': 0.5, 'features__pipe__vect__min_df': 2, 'features__pipe__vect__ngram_range': (1, 1)}\n",
      "0.562883 (0.009866) with: {'clf__alpha': 0.25, 'features__pipe__vect__max_df': 0.5, 'features__pipe__vect__min_df': 2, 'features__pipe__vect__ngram_range': (1, 2)}\n",
      "0.556602 (0.009848) with: {'clf__alpha': 0.25, 'features__pipe__vect__max_df': 0.75, 'features__pipe__vect__min_df': 1, 'features__pipe__vect__ngram_range': (1, 1)}\n",
      "0.540153 (0.012140) with: {'clf__alpha': 0.25, 'features__pipe__vect__max_df': 0.75, 'features__pipe__vect__min_df': 1, 'features__pipe__vect__ngram_range': (1, 2)}\n",
      "0.558397 (0.010251) with: {'clf__alpha': 0.25, 'features__pipe__vect__max_df': 0.75, 'features__pipe__vect__min_df': 2, 'features__pipe__vect__ngram_range': (1, 1)}\n",
      "0.562883 (0.009866) with: {'clf__alpha': 0.25, 'features__pipe__vect__max_df': 0.75, 'features__pipe__vect__min_df': 2, 'features__pipe__vect__ngram_range': (1, 2)}\n",
      "0.557948 (0.010769) with: {'clf__alpha': 0.5, 'features__pipe__vect__max_df': 0.25, 'features__pipe__vect__min_df': 1, 'features__pipe__vect__ngram_range': (1, 1)}\n",
      "0.525049 (0.008579) with: {'clf__alpha': 0.5, 'features__pipe__vect__max_df': 0.25, 'features__pipe__vect__min_df': 1, 'features__pipe__vect__ngram_range': (1, 2)}\n",
      "0.569015 (0.009115) with: {'clf__alpha': 0.5, 'features__pipe__vect__max_df': 0.25, 'features__pipe__vect__min_df': 2, 'features__pipe__vect__ngram_range': (1, 1)}\n",
      "0.566323 (0.010714) with: {'clf__alpha': 0.5, 'features__pipe__vect__max_df': 0.25, 'features__pipe__vect__min_df': 2, 'features__pipe__vect__ngram_range': (1, 2)}\n",
      "0.557948 (0.010769) with: {'clf__alpha': 0.5, 'features__pipe__vect__max_df': 0.5, 'features__pipe__vect__min_df': 1, 'features__pipe__vect__ngram_range': (1, 1)}\n",
      "0.525049 (0.008579) with: {'clf__alpha': 0.5, 'features__pipe__vect__max_df': 0.5, 'features__pipe__vect__min_df': 1, 'features__pipe__vect__ngram_range': (1, 2)}\n",
      "0.569015 (0.009115) with: {'clf__alpha': 0.5, 'features__pipe__vect__max_df': 0.5, 'features__pipe__vect__min_df': 2, 'features__pipe__vect__ngram_range': (1, 1)}\n",
      "0.566323 (0.010714) with: {'clf__alpha': 0.5, 'features__pipe__vect__max_df': 0.5, 'features__pipe__vect__min_df': 2, 'features__pipe__vect__ngram_range': (1, 2)}\n",
      "0.557948 (0.010769) with: {'clf__alpha': 0.5, 'features__pipe__vect__max_df': 0.75, 'features__pipe__vect__min_df': 1, 'features__pipe__vect__ngram_range': (1, 1)}\n",
      "0.525049 (0.008579) with: {'clf__alpha': 0.5, 'features__pipe__vect__max_df': 0.75, 'features__pipe__vect__min_df': 1, 'features__pipe__vect__ngram_range': (1, 2)}\n",
      "0.569015 (0.009115) with: {'clf__alpha': 0.5, 'features__pipe__vect__max_df': 0.75, 'features__pipe__vect__min_df': 2, 'features__pipe__vect__ngram_range': (1, 1)}\n",
      "0.566323 (0.010714) with: {'clf__alpha': 0.5, 'features__pipe__vect__max_df': 0.75, 'features__pipe__vect__min_df': 2, 'features__pipe__vect__ngram_range': (1, 2)}\n",
      "0.549275 (0.010277) with: {'clf__alpha': 0.75, 'features__pipe__vect__max_df': 0.25, 'features__pipe__vect__min_df': 1, 'features__pipe__vect__ngram_range': (1, 1)}\n",
      "0.513534 (0.013629) with: {'clf__alpha': 0.75, 'features__pipe__vect__max_df': 0.25, 'features__pipe__vect__min_df': 1, 'features__pipe__vect__ngram_range': (1, 2)}\n",
      "0.570959 (0.013058) with: {'clf__alpha': 0.75, 'features__pipe__vect__max_df': 0.25, 'features__pipe__vect__min_df': 2, 'features__pipe__vect__ngram_range': (1, 1)}\n",
      "0.557201 (0.008052) with: {'clf__alpha': 0.75, 'features__pipe__vect__max_df': 0.25, 'features__pipe__vect__min_df': 2, 'features__pipe__vect__ngram_range': (1, 2)}\n",
      "0.549275 (0.010277) with: {'clf__alpha': 0.75, 'features__pipe__vect__max_df': 0.5, 'features__pipe__vect__min_df': 1, 'features__pipe__vect__ngram_range': (1, 1)}\n",
      "0.513534 (0.013629) with: {'clf__alpha': 0.75, 'features__pipe__vect__max_df': 0.5, 'features__pipe__vect__min_df': 1, 'features__pipe__vect__ngram_range': (1, 2)}\n",
      "0.570959 (0.013058) with: {'clf__alpha': 0.75, 'features__pipe__vect__max_df': 0.5, 'features__pipe__vect__min_df': 2, 'features__pipe__vect__ngram_range': (1, 1)}\n",
      "0.557201 (0.008052) with: {'clf__alpha': 0.75, 'features__pipe__vect__max_df': 0.5, 'features__pipe__vect__min_df': 2, 'features__pipe__vect__ngram_range': (1, 2)}\n",
      "0.549275 (0.010277) with: {'clf__alpha': 0.75, 'features__pipe__vect__max_df': 0.75, 'features__pipe__vect__min_df': 1, 'features__pipe__vect__ngram_range': (1, 1)}\n",
      "0.513534 (0.013629) with: {'clf__alpha': 0.75, 'features__pipe__vect__max_df': 0.75, 'features__pipe__vect__min_df': 1, 'features__pipe__vect__ngram_range': (1, 2)}\n",
      "0.570959 (0.013058) with: {'clf__alpha': 0.75, 'features__pipe__vect__max_df': 0.75, 'features__pipe__vect__min_df': 2, 'features__pipe__vect__ngram_range': (1, 1)}\n",
      "0.557201 (0.008052) with: {'clf__alpha': 0.75, 'features__pipe__vect__max_df': 0.75, 'features__pipe__vect__min_df': 2, 'features__pipe__vect__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "# MultinomialNB\n",
    "best_mnb_countvect = grid_vect(mnb, parameters_mnb, X_train, X_test, parameters_text=parameters_vect, vect=countvect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfvect = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['features', 'clf']\n",
      "parameters:\n",
      "{'clf__alpha': (0.25, 0.5, 0.75),\n",
      " 'features__pipe__vect__max_df': (0.25, 0.5, 0.75),\n",
      " 'features__pipe__vect__min_df': (1, 2),\n",
      " 'features__pipe__vect__ngram_range': ((1, 1), (1, 2))}\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed:   21.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 22.154s\n",
      "\n",
      "Best CV score: 0.545\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.25\n",
      "\tfeatures__pipe__vect__max_df: 0.25\n",
      "\tfeatures__pipe__vect__min_df: 2\n",
      "\tfeatures__pipe__vect__ngram_range: (1, 1)\n",
      "Test score with best_estimator_: 0.528\n",
      "\n",
      "\n",
      "Classification Report Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.53      0.89      0.67       284\n",
      "anticipation       0.22      0.05      0.08        95\n",
      "     disgust       0.00      0.00      0.00        48\n",
      "        fear       0.39      0.11      0.17        66\n",
      "         joy       0.57      0.68      0.62       187\n",
      "        love       0.00      0.00      0.00         1\n",
      "    optimism       0.00      0.00      0.00        22\n",
      "   pessimism       0.00      0.00      0.00        16\n",
      "     sadness       0.00      0.00      0.00        22\n",
      "    surprise       0.00      0.00      0.00         3\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       744\n",
      "   macro avg       0.17      0.17      0.15       744\n",
      "weighted avg       0.41      0.53      0.44       744\n",
      "\n",
      "all results\n",
      "0.527441 (0.008736) with: {'clf__alpha': 0.25, 'features__pipe__vect__max_df': 0.25, 'features__pipe__vect__min_df': 1, 'features__pipe__vect__ngram_range': (1, 1)}\n",
      "0.465979 (0.007316) with: {'clf__alpha': 0.25, 'features__pipe__vect__max_df': 0.25, 'features__pipe__vect__min_df': 1, 'features__pipe__vect__ngram_range': (1, 2)}\n",
      "0.545087 (0.011938) with: {'clf__alpha': 0.25, 'features__pipe__vect__max_df': 0.25, 'features__pipe__vect__min_df': 2, 'features__pipe__vect__ngram_range': (1, 1)}\n",
      "0.531030 (0.011100) with: {'clf__alpha': 0.25, 'features__pipe__vect__max_df': 0.25, 'features__pipe__vect__min_df': 2, 'features__pipe__vect__ngram_range': (1, 2)}\n",
      "0.527441 (0.008736) with: {'clf__alpha': 0.25, 'features__pipe__vect__max_df': 0.5, 'features__pipe__vect__min_df': 1, 'features__pipe__vect__ngram_range': (1, 1)}\n",
      "0.465979 (0.007316) with: {'clf__alpha': 0.25, 'features__pipe__vect__max_df': 0.5, 'features__pipe__vect__min_df': 1, 'features__pipe__vect__ngram_range': (1, 2)}\n",
      "0.545087 (0.011938) with: {'clf__alpha': 0.25, 'features__pipe__vect__max_df': 0.5, 'features__pipe__vect__min_df': 2, 'features__pipe__vect__ngram_range': (1, 1)}\n",
      "0.531030 (0.011100) with: {'clf__alpha': 0.25, 'features__pipe__vect__max_df': 0.5, 'features__pipe__vect__min_df': 2, 'features__pipe__vect__ngram_range': (1, 2)}\n",
      "0.527441 (0.008736) with: {'clf__alpha': 0.25, 'features__pipe__vect__max_df': 0.75, 'features__pipe__vect__min_df': 1, 'features__pipe__vect__ngram_range': (1, 1)}\n",
      "0.465979 (0.007316) with: {'clf__alpha': 0.25, 'features__pipe__vect__max_df': 0.75, 'features__pipe__vect__min_df': 1, 'features__pipe__vect__ngram_range': (1, 2)}\n",
      "0.545087 (0.011938) with: {'clf__alpha': 0.25, 'features__pipe__vect__max_df': 0.75, 'features__pipe__vect__min_df': 2, 'features__pipe__vect__ngram_range': (1, 1)}\n",
      "0.531030 (0.011100) with: {'clf__alpha': 0.25, 'features__pipe__vect__max_df': 0.75, 'features__pipe__vect__min_df': 2, 'features__pipe__vect__ngram_range': (1, 2)}\n",
      "0.493644 (0.012101) with: {'clf__alpha': 0.5, 'features__pipe__vect__max_df': 0.25, 'features__pipe__vect__min_df': 1, 'features__pipe__vect__ngram_range': (1, 1)}\n",
      "0.415882 (0.004556) with: {'clf__alpha': 0.5, 'features__pipe__vect__max_df': 0.25, 'features__pipe__vect__min_df': 1, 'features__pipe__vect__ngram_range': (1, 2)}\n",
      "0.527591 (0.012073) with: {'clf__alpha': 0.5, 'features__pipe__vect__max_df': 0.25, 'features__pipe__vect__min_df': 2, 'features__pipe__vect__ngram_range': (1, 1)}\n",
      "0.502617 (0.011673) with: {'clf__alpha': 0.5, 'features__pipe__vect__max_df': 0.25, 'features__pipe__vect__min_df': 2, 'features__pipe__vect__ngram_range': (1, 2)}\n",
      "0.493644 (0.012101) with: {'clf__alpha': 0.5, 'features__pipe__vect__max_df': 0.5, 'features__pipe__vect__min_df': 1, 'features__pipe__vect__ngram_range': (1, 1)}\n",
      "0.415882 (0.004556) with: {'clf__alpha': 0.5, 'features__pipe__vect__max_df': 0.5, 'features__pipe__vect__min_df': 1, 'features__pipe__vect__ngram_range': (1, 2)}\n",
      "0.527591 (0.012073) with: {'clf__alpha': 0.5, 'features__pipe__vect__max_df': 0.5, 'features__pipe__vect__min_df': 2, 'features__pipe__vect__ngram_range': (1, 1)}\n",
      "0.502617 (0.011673) with: {'clf__alpha': 0.5, 'features__pipe__vect__max_df': 0.5, 'features__pipe__vect__min_df': 2, 'features__pipe__vect__ngram_range': (1, 2)}\n",
      "0.493644 (0.012101) with: {'clf__alpha': 0.5, 'features__pipe__vect__max_df': 0.75, 'features__pipe__vect__min_df': 1, 'features__pipe__vect__ngram_range': (1, 1)}\n",
      "0.415882 (0.004556) with: {'clf__alpha': 0.5, 'features__pipe__vect__max_df': 0.75, 'features__pipe__vect__min_df': 1, 'features__pipe__vect__ngram_range': (1, 2)}\n",
      "0.527591 (0.012073) with: {'clf__alpha': 0.5, 'features__pipe__vect__max_df': 0.75, 'features__pipe__vect__min_df': 2, 'features__pipe__vect__ngram_range': (1, 1)}\n",
      "0.502617 (0.011673) with: {'clf__alpha': 0.5, 'features__pipe__vect__max_df': 0.75, 'features__pipe__vect__min_df': 2, 'features__pipe__vect__ngram_range': (1, 2)}\n",
      "0.464035 (0.008286) with: {'clf__alpha': 0.75, 'features__pipe__vect__max_df': 0.25, 'features__pipe__vect__min_df': 1, 'features__pipe__vect__ngram_range': (1, 1)}\n",
      "0.397039 (0.002282) with: {'clf__alpha': 0.75, 'features__pipe__vect__max_df': 0.25, 'features__pipe__vect__min_df': 1, 'features__pipe__vect__ngram_range': (1, 2)}\n",
      "0.507103 (0.011998) with: {'clf__alpha': 0.75, 'features__pipe__vect__max_df': 0.25, 'features__pipe__vect__min_df': 2, 'features__pipe__vect__ngram_range': (1, 1)}\n",
      "0.474503 (0.010553) with: {'clf__alpha': 0.75, 'features__pipe__vect__max_df': 0.25, 'features__pipe__vect__min_df': 2, 'features__pipe__vect__ngram_range': (1, 2)}\n",
      "0.464035 (0.008286) with: {'clf__alpha': 0.75, 'features__pipe__vect__max_df': 0.5, 'features__pipe__vect__min_df': 1, 'features__pipe__vect__ngram_range': (1, 1)}\n",
      "0.397039 (0.002282) with: {'clf__alpha': 0.75, 'features__pipe__vect__max_df': 0.5, 'features__pipe__vect__min_df': 1, 'features__pipe__vect__ngram_range': (1, 2)}\n",
      "0.507103 (0.011998) with: {'clf__alpha': 0.75, 'features__pipe__vect__max_df': 0.5, 'features__pipe__vect__min_df': 2, 'features__pipe__vect__ngram_range': (1, 1)}\n",
      "0.474503 (0.010553) with: {'clf__alpha': 0.75, 'features__pipe__vect__max_df': 0.5, 'features__pipe__vect__min_df': 2, 'features__pipe__vect__ngram_range': (1, 2)}\n",
      "0.464035 (0.008286) with: {'clf__alpha': 0.75, 'features__pipe__vect__max_df': 0.75, 'features__pipe__vect__min_df': 1, 'features__pipe__vect__ngram_range': (1, 1)}\n",
      "0.397039 (0.002282) with: {'clf__alpha': 0.75, 'features__pipe__vect__max_df': 0.75, 'features__pipe__vect__min_df': 1, 'features__pipe__vect__ngram_range': (1, 2)}\n",
      "0.507103 (0.011998) with: {'clf__alpha': 0.75, 'features__pipe__vect__max_df': 0.75, 'features__pipe__vect__min_df': 2, 'features__pipe__vect__ngram_range': (1, 1)}\n",
      "0.474503 (0.010553) with: {'clf__alpha': 0.75, 'features__pipe__vect__max_df': 0.75, 'features__pipe__vect__min_df': 2, 'features__pipe__vect__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "best_mnb_tfidf = grid_vect(mnb, parameters_mnb, X_train, X_test, parameters_text=parameters_vect, vect=tfidfvect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_df: 0.25 or maximum document frequency of 25%.\n",
    "#min_df: 2 or the words need to appear in at least 2 tweets\n",
    "#ngram_range: (1, 1)\n",
    "#clf__alpha: 0.75\n",
    "textcountscols = ['count_capital_words','count_emojis','count_excl_quest_marks','count_hashtags'\n",
    "                      ,'count_mentions','count_urls','count_words']\n",
    "    \n",
    "features = FeatureUnion([('textcounts', ColumnExtractor(cols=textcountscols))\n",
    "                         , ('pipe', Pipeline([('cleantext', ColumnExtractor(cols='clean_text'))\n",
    "                                              , ('vect', CountVectorizer(max_df=0.25, min_df=2, ngram_range=(1,1)))]))]\n",
    "                       , n_jobs=-1)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', features)\n",
    "    , ('clf', MultinomialNB(alpha=0.75))\n",
    "])\n",
    "\n",
    "best_model = pipeline.fit(df_model.drop('sentiment', axis=1), df_model.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['joy', 'joy', 'joy']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_pos = pd.read_pickle('../data/df_model_pos.p')\n",
    "best_model.predict(df_model_pos).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anger', 'joy', 'anger', 'anger']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_neg = pd.read_pickle('../data/df_model_neg.p')\n",
    "best_model.predict(df_model_neg).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
