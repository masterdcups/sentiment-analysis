{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import reuters\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import os\n",
    "import spacy\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense, Embedding\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pad_vec_sequences(sequences, maxlen=40):\n",
    "        new_sequences = []\n",
    "        for sequence in sequences:\n",
    "\n",
    "            orig_len, vec_len = np.shape(sequence)\n",
    "\n",
    "\n",
    "\n",
    "            if orig_len < maxlen:\n",
    "                new = np.zeros((maxlen, vec_len))\n",
    "                for k in range(maxlen-orig_len,maxlen):\n",
    "                    new[k:, :] = sequence[k-maxlen+orig_len]\n",
    "\n",
    "            else:\n",
    "                new = np.zeros((maxlen, vec_len))\n",
    "                for k in range(0,maxlen):\n",
    "                    new[k:,:] = sequence[k]\n",
    "\n",
    "            new_sequences.append(new)\n",
    "\n",
    "        return np.array(new_sequences)\n",
    "\n",
    "def save_model(model, filename):\n",
    "\n",
    "    model_json = model.to_json()\n",
    "    with open(filename + '.model', \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "        json_file.close();\n",
    "    model.save_weights(filename + \".weights\")\n",
    "\n",
    "\n",
    "def load_model(filename):\n",
    "\n",
    "    json_file = open(filename + '.model', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    loaded_model.load_weights(filename + \".weights\")\n",
    "\n",
    "    return loaded_model;\n",
    "\n",
    "def build_dict(f, grams):\n",
    "    dic = Counter()\n",
    "    for sentence in open(f).xreadlines():\n",
    "        dic.update(tokenize(sentence, grams))\n",
    "    return dic\n",
    "\n",
    "def tokenize(line):\n",
    "    words = word_tokenize(line)\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LSTM network using glove2vec word vector embeddings as initializations to embedding layer \n",
    "# of LSTM\n",
    "def model4():\n",
    "    \n",
    "    tweet = pd.read_pickle('../data/final/NotIot_clean_FR_news_lexic.p')\n",
    "    tweet = tweet[['clean_text', 'sentiment']]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(tweet.clean_text, tweet.sentiment, test_size=0.1, random_state=37)\n",
    "    print('# Train data samples:', X_train.shape[0])\n",
    "    print('# Test data samples:', X_test.shape[0])\n",
    "    assert X_train.shape[0] == y_train.shape[0]\n",
    "    assert X_test.shape[0] == y_test.shape[0]\n",
    "    \n",
    "    MAX_NB_WORDS = 100\n",
    "\n",
    "        \n",
    "    #tokenize tweet text\n",
    "    tk = Tokenizer(num_words=MAX_NB_WORDS,lower=True, split=\" \")\n",
    "    tk.fit_on_texts(X_train)\n",
    "    word_index = tk.word_index\n",
    "    #convert text into sequence\n",
    "    X_train_seq = tk.texts_to_sequences(X_train)\n",
    "    X_test_seq = tk.texts_to_sequences(X_test)\n",
    "    #normalise the length of each sequence\n",
    "    seq_lengths = X_train.apply(lambda x: len(x.split(' ')))\n",
    "    print(seq_lengths.describe())\n",
    "\n",
    "\n",
    "    from keras.layers import Embedding\n",
    "\n",
    "    MAX_SEQUENCE_LENGTH = 50\n",
    "    EMBEDDING_DIM = 100\n",
    "    embeddings_index = {}\n",
    "    f = open(os.path.join('data/', 'glove.6B.100d.txt'))\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    embedding_layer = Embedding(input_dim=len(word_index) + 1,\n",
    "                                output_dim=EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "\n",
    "    num_classes = 6\n",
    "\n",
    "    max_words=MAX_SEQUENCE_LENGTH\n",
    "\n",
    "    y_train = keras.utils.to_categorical(labels, num_classes)\n",
    "\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    print(\"Training data: \")\n",
    "    print(data.shape),(y_train.shape)\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "\n",
    "\n",
    "    model.add(LSTM(128, return_sequences=False\n",
    "               , input_shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)))\n",
    "\n",
    "\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(data,y_train,epochs=20, batch_size=100, verbose=1)\n",
    "    save_model(model,\"/tmp/model4\")\n",
    "\n",
    "    model.evaluate(X_test,y_test)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
